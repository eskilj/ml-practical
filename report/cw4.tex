\documentclass[]{article}

%opening
\title{MLP Coursework 4}
\author{Eskil Joergensen}
\date{\today}

\usepackage[parfill]{parskip}
\usepackage{pgfgantt}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{tablefootnote}
\usepackage{tikz}
\usepackage{pgfplots}
\newcommand*\rot{\rotatebox{90}}
\usepackage{graphicx}
\graphicspath{ {images/} }

\usepackage{amsmath}

\usepackage{booktabs}
\usepackage[font={small,it}]{caption}

\begin{document}

\maketitle

\section{Introduction}

Fully connected neural networks are trained at a relatively low computational cost, and they have been used in machine learning for quite some time. However, current state-of-the-art networks are all convolutional neural networks (CNNs) \cite{long2015fully}. The high performance is (typically) achieved as the kernels in the convolutional layers extract features from the input data via their spacial locality. A common use case is object recognition in images, but can also be used in other tasks such as natural language processing \cite{Collobert}. 

Preliminary work has been done on the same data set using a fully connected neural network. More details are presented in the Background section (\ref{background}) below. 

CNNs introduce a significantly larger number of weights and connections, compared to a fully connected network, which means the parameters of the network must be carefully tuned to ensure adequately successful training. 

This report presents a series of experiments set to investigate how some key components of a CNN affect it's overall performance during training. Furthermore, the (optimal) findings from each experiment are integrated into the network for the subsequent experiments, trying to improve the performance of the network as much as possible. The following aspects of the CNN are investigated:

\begin{itemize}
	
	\item How does the size of kernels and number of feature maps in the convolutional layers affect the training? Time, performance, computational resources
	
	\item How does the choice of activation function impact the performance of the network?
	
	\item What type, and magnitude, of regularization is needed to overcome potential over-fitting of the CNN? Without any normalization or regularization significant over-fitting can occur during training of the network. MORE +++
	
\end{itemize}

optimize the training performance of a CNN on a image classification task, using the CIFAR 10 dataset \cite{cifar10}. In addition to improving the overall performance of the training of the network, the experiments were guided by a goal of 70\% accuracy on the test data set. As a result, the methodology and incremental improvements to the model were made on these grounds. 

\subsection{Background} \label{background}

Preliminary experiments have been conducted on the same data, using a fully connected neural network. The network was trained using a variety of activation functions, hidden layer depths and widths as well as learning rate scheduler. 

\subsubsection{Activation Functions}

Four activation functions were compared. The logistic sigmoid $f(x)=\frac{1}{1+e^{-x}}$, the hyperbolic tangent $f(x)=tanh(x)$ the rectified linear (ReLu) $f(x)=max(0,x)$ and the exponential linear unit (ELU):

\begin{equation} \label{eq:elu}
f(x) = \begin{cases}
x &\text{if $x >$ 0}\\ \alpha (\text{exp}(x) - 1) &\text{if  $x \leq$ 0}
\end{cases}
\end{equation}

where \(\alpha\) is a hyperparameter that decides the range of negative values for which the activation function saturates.

The results showed that the ELU activation function outperformed the other three, with a final training and validation set accuracy of 66.9 \% and 50.5\% respectively. 

\subsubsection{Hidden layers}

\subsubsection{Learning Rate Schedule}

\section{Network Architecture}



\section{Methodology}

2. Treatment for the object (what do to object)
3. Procedure to collect data
4. Procedure to analyze data


\subsection{The Network Architecture}

A neural network with two convolutional and two fully connected hidden layers was chosen for this study. The network also had two max pooling layers, one after each convolutional layer, as well as a 10-way softmax output classification. 

Why this model?

\begin{figure}[h]
	\includegraphics[width=\textwidth]{conv}
	\caption{Showing a visual representation of the model used, from TensorBoard.}
	\label{conv}
	\centering
\end{figure}

\subsubsection{Convolutional Layer}

The convolution was implemented using the \texttt{tf.nn.conv2d} method from TensorFlow (tf) with stride of 1 and no padding (\texttt{padding='SAME'}). The convolution operation was executed on the layer inputs together with the kernels (using the specified stride and padding). Both the layer inputs and the kernels were TensorFlow tensors, and the result of the convolutional operation was also a TensorFlow tensor. 

The biases were added to the tensor after the convolution. In the convolutional layers where batch normalization was specified, ... . Finally, an activation function was applied to the tensor, before passing the output on to the next layer. 

A truncated normal distribution was use to initialize the kernels, and the biases were initialized to zero. See Section \ref{weights}, Weight and Biases, for the specific implementation in TensorFlow. 

\subsubsection{Fully Connected and Output prediction Layer}

The fully connected and the classification output layers were implemented using the same class, as only a few factors separate the behavior of the two. A constant halving of the number of hidden units were integrated into all the fully connected layers, except the output prediction layer which had a 10-was output.

The weights and biases were initialized in the same as mentioned for the convolutional layers, only the weights for the affine layers were two dimensional. 

Finally, the inputs were multiplied with the weights using \texttt{tf.matmul} ,before the biases were added to the tensor. For the final classification layer this tensor would be returned, otherwise an activation function would be applied before returning the tensor.

\subsubsection{Pooling Layer}

The max pooling layer was implemented using the \texttt{tf.nn.max\_pool} method from TensorFlow. Each pooling layer used a 3 by 3 window for the sampling, and a stride of 2. Similarly to the convolutional layer the pooling layer used no padding, \texttt{padding='SAME'} in TensorFlow. 

\begin{figure}[h]
	\centering
	\includegraphics[width=0.6\textwidth]{pool}
	\caption{Showing an example of a 2x2 max pool operation. The 3x3 equivalent operation was used throughout this investigation.}
	\label{pool}
	\centering
\end{figure}

\subsubsection{Dropout Layer}

The dropout layer was implemented using \texttt{tf.nn.dropout(inputs, keep\_prop)}, and will keep each element in the input tensor with a probability of \texttt{keep\_prob}. 

\subsubsection{Batch Normalization}

The batch normalization of the convolutional layer was achieved with the \texttt{tf.nn.max\_pool} method from TensorFlow. Batch normalization transforms its input to zero mean and unit variance. By statistical analysis the batch normalization normalizes the inputs of a transformation layer. Equation \ref{batch_eq} shows how the inputs \(x_i\) are normalized

\begin{equation} \label{batch_eq}
BN(x_i) = \gamma_i \Big( \frac{x_i - \mu_i}{\sqrt{\sigma_i^2 + \epsilon}} \Big) + \beta_i
\end{equation}

where \(\mu_i\) is the batch mean, \(\sigma_i^2\) is the batch variance, \(\gamma_i\) is a scale factor and \(\beta_i\) is a shift factor. \(\gamma_i\) was set to 1.0, \(\beta_i\) to zero and \(\epsilon\) to 1e-3. The batch mean and variance was obtained from the inputs using \texttt{tf.nn.moments}. 


\subsubsection{Weights and Biases} \label{weights}

The weights used in both the convolutional and fully connected layers were initialized using a truncated normal distribution. The values were distributed with zero mean and a standard deviation of 0.1. Values more than two standard deviations away from the mean were dropped. The distributions were initialized with the \texttt{tf.truncated\_normal} method. 

The biases used were all initialized to zero using the tf.zeros method. Additionally, the weights and biases were added to the TensorFlow. graph using  tf.Variable().

\subsection{Experimental Procedure}

The procedure used to tune the performance of the network involved three stages. During the initial stage the network was trained for few epochs, with a coarse spread of hyper parameter and multiple algorithms were tested during this phase. The overall results from the initial trials informed the finer range of parameters used for additional training and further tuning of hyper parameters. The second stage involved longer training. The third stage, before the final testing, the model was trained for significantly longer, and only three varieties of the model were used. 

The default baseline model used has two convolutional layers, two fully-connected layers and finally a softmax classification layer. The network also had two 2x2 pooling layers after each convolutional layer. 

Instead of a learning rate scheduler, the Adam adaptive learning rate optimizer was used throughout the investigation. The optimizer was used with varying learning rates, but with beta1=0.9, beta2=0.999 and epsilon = 1e-8. (default parameters) 

\subsubsection{Activation Functions}

The model was trained using three learning rates, with the four activation functions used in the preliminary experiments: ReLu, Elu, Tahn and Sigmoid. During the first round of trials the network was trained for 5 epochs. A relatively large, non-uniform spread of the learning rate was chosen intentionally to observe how the activation functions would react to the different rates. 

After the initial trials, the best two activation functions were kept for further experimentation, and the other two were discarded. The learning rates were also narrowed down, closer to the best performing learning rate. All activation functions, learning rates and training epochs used during the investigation can be seen in Table \ref{ac-overview}.

Again, after the second round, the learning rates were modified before the last round of training. As the two last activation functions performed similarly, both were tested in the final trials. The combination of learning rate and activation functions, that resulted in best performance, was then used for the during the next set of experiments, see Section \ref{filter-methods}.

\begin{table}[h]
	\centering
	\caption{Showing the combinations of activation functions, learning rates and training duration in Epochs, for the three rounds of experimentation.}
	\label{ac-overview}
	\begin{tabular}{@{}cccc@{}}
		\toprule
		Round & Learning Rates & Activation Fn. & Training Epochs \\ \midrule
		1 & 5e-4, 3e-3, 0.01 & ReLu, Elu, Tanh, Sigmoid & 5 \\
		2 & 1e-4, 9e-4 & ReLu, Elu & 10 \\
		3 & 4e-4, 7e-4 & ReLu, Elu & 15 \\ \bottomrule
	\end{tabular}
\end{table}

The effect of the learning rates and non-linear activation functions on the aforementioned network were tested with 3x3 kernels and 24 feature maps. The network did not use any kind of L2 regularization, batch normalization or drop out. 

The procedure used to train the network is presented in Section .

\subsubsection{Filter Sizes and Feature Maps} \label{filter-methods} 

During the second part of the experimentation, the network was trained with nine combinations of kernel sizes and feature maps. The size of kernels were 3, 5 and 7, whereas the number of feature maps were 16, 24 and 32. During the first round the network was trained for 5 epoch, using the ELU activation function was used with a learning rate of 0.0004. 

The network was supposed to be trained in three rounds, incrementally determining the best combination of kernel size and number of feature maps. However, due to exceedingly long training times and swap memory warnings (during the two most expensive trials) the two later rounds were not completed. 

The three combinations with 32 feature maps all performed similarly, but at very different completion times and computational cost. See Table \ref{filter-res} for execution times. The 3x3 kernel and 32 feature maps were used for the remaining experimentation. 

\subsubsection{Regularization and Normalization}

The last set of experiments 

As with the previous experiments, short first round trials were run using coarse set of parameters. The network was trained using eight different combinations of batch normalization, drop out and L2 weight decay. The regularization and normalization parameters used in all three rounds can be seen in Table \ref{reg-table}. The batch normalization only applied to the convolutional layers and the dropout 

Using three nested loops, the network was trained using eight different combinations of batch normalization, drop out and L2 weight decay. The batch normalization added to the convolutional layers were either true or false, the L2 decay was none or 0.0005 and the keep probability of the dropout layers were either 1.0 (keep all) or 0.5. The network was trained for 5 epochs during the first set of trials. 

The third round of training used 20 epochs instead of 15, as the regularization slowed down the training.

\begin{table}[h]
	\centering
	\caption{Showing the combinations of parameters used during the normalization and regularization investigative rounds.}
	\label{reg-table}
	\begin{tabular}{@{}ccccc@{}}
		\toprule
		Round & Batch Norm. & Dropout Prob. & L2 Reg. & Training Epochs \\ \midrule
		1 & True, False & None, 0.5 & None, 5e-4 & 5 \\
		2 & True & 0.45, 0.55 & 5e-4 & 10 \\
		3 & True & 0.5 & 5e-4, None & 20 \\ \bottomrule
	\end{tabular}
\end{table}

The training runs were done with 3x3 kernels and 32 feature maps in each of the convolutional layers. 

\subsection{Network Training and Data Collection}

placeholder data, batch size 50

TensorBoard 
Normalized running means - numpy
custom model class
evaluate validation every 100 step, plus first and last step
\texttt{tf.softmax\_cross\_entropy\_with\_logits}

\texttt{tf.train.AdamOptimizer}
 
\section{Results and Discussion}

All tests were performed on a macOS Sierra 10.12. operating system, with an Intel i5 2.6 GHz processor and 8 GB DDR3 memory with 1600 MHz. All trials were executed inside a custom mlp Python 2.7 virtual environment. 

\subsection{Activation Functions}

Training the network with the linear unit activation functions, ReLu and ELU, resulted in the stronger overall performance compared to the training outcome with the sigmoid or hyperbolic tangent. 

\begin{table}[h]
	\centering
	\caption{Showing the final validation classification accuracy  for the four activation functions, after 5 epochs, for different learning rates. The classification accuracies are reported as percentages.}
	\label{ac-table}
	\begin{tabular}{@{}c|cccc@{}}
		\toprule
		Learning Rate & ELU & ReLu & Tanh & Sigmoid \\ \midrule
		5e-4 & 69.5 & 70.27 & 67.7 & 46.9 \\
		3e-3 & 62.2 & 61.3 & 10.0 & 10.0 \\
		0.01 & 10.0 & 10.0 & 10.0 & 10.0 \\ \bottomrule
	\end{tabular}
\end{table}

The lowest learning rate, 0.0005, resulted in the highest classification accuracies across all the activation functions, and the ReLu trial had a final validation accuracy of 70.3\%. Training the network with a learning rate of 0.01 resulted in high total error and very low accuracies. In a 10-way classification task, 10\% accuracy means the final output are guesses, and no (useful) learning has occurred.

The training of the network during the final two rounds resulted in comparable final error and classification accuracies, see Figure \ref{ac_res}. The highest validation accuracy was 71.6\%, by ELU with the learning rate of 0.0004, and the lowest was 69.7\% by ReLu with the learning rate of 0.0007. Although the validation accuracy was over 70\%, the training set accuracies were much higher, which indicates a high degree of over-fitting. The over-fitting was also reflected in the validation set errors.

\begin{figure}[h]
	\includegraphics[width=\textwidth]{ac_res}
	\caption{Showing a visual representation of the model used, from TensorBoard.}
	\label{ac_res}
	\centering
\end{figure}

Compared to the training of the fully connected network, presented in Section \ref{background}, the choice of the activation functions effect the training performance of the CNN and the fully connected network in similar way. 

\subsection{Filter Sizes and Feature Maps}

The number of feature maps had a greater impact on the training performance, compared to the size of the kernels. As shown in Figure , the three best performing trials were all using convolutional layers with 32 feature maps. 

\begin{table}[h]
	\centering
	\caption{Showing the training execution time, in minutes, for 5 epochs for different kernel sizes and number of feature maps in the convolutional layers.}
	\label{filter-res}
	\begin{tabular}{@{}ccccc@{}}
		\toprule
		\multicolumn{2}{c}{Feature Maps} & 16 & 24 & 32 \\ \midrule
		\multirow{3}{*}{\rot{Size}} & \multicolumn{1}{c|}{3x3} & 12.5 & 19.1 & 27.0 \\
		& \multicolumn{1}{c|}{5x5} & 22.5 & 34.6 & 49.1 \\
		& \multicolumn{1}{c|}{7x7} & 39.9 & 57.9 & 67.0 \\ \bottomrule
	\end{tabular}
\end{table}

\subsubsection{Regularization and Normalization}



\section{Discussion}


\section{Conclusion}

\section{Future Work}


\clearpage
\medskip
\bibliographystyle{IEEEtran}
\bibliography{ref.bib}

\end{document}
