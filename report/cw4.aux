\relax 
\citation{long2015fully}
\citation{Collobert}
\citation{cifar10}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Background}{2}}
\newlabel{background}{{1.1}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1}Activation Functions}{2}}
\newlabel{eq:elu}{{1}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.2}Hidden layers}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.3}Learning Rate Schedule}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Network Architecture}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methodology}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}The Network Architecture}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Showing a visual representation of the model used, from TensorBoard.\relax }}{3}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{conv}{{1}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Convolutional Layer}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Fully Connected and Output prediction Layer}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}Pooling Layer}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Showing an example of a 2x2 max pool operation. The 3x3 equivalent operation was used throughout this investigation.\relax }}{4}}
\newlabel{pool}{{2}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.4}Dropout Layer}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.5}Batch Normalization}{4}}
\newlabel{batch_eq}{{2}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.6}Weights and Biases}{5}}
\newlabel{weights}{{3.1.6}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Experimental Procedure}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Activation Functions}{5}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Showing the combinations of activation functions, learning rates and training duration in Epochs, for the three rounds of experimentation.\relax }}{6}}
\newlabel{ac-overview}{{1}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Filter Sizes and Feature Maps}{6}}
\newlabel{filter-methods}{{3.2.2}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Regularization and Normalization}{7}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Showing the combinations of parameters used during the normalization and regularization investigative rounds.\relax }}{7}}
\newlabel{reg-table}{{2}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Network Training and Data Collection}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results and Discussion}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Activation Functions}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Showing a visual representation of the model used, from TensorBoard.\relax }}{8}}
\newlabel{ac_res}{{3}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Filter Sizes and Feature Maps}{9}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Showing the training execution time, in minutes, for 5 epochs for different kernel sizes and number of feature maps in the convolutional layers.\relax }}{9}}
\newlabel{filter-res}{{3}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Regularization and Normalization}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Future Work}{9}}
\bibstyle{IEEEtran}
\bibdata{ref.bib}
\bibcite{long2015fully}{1}
\bibcite{Collobert}{2}
\bibcite{cifar10}{3}
