\relax 
\citation{long2015fully}
\citation{Collobert}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Background}{2}}
\newlabel{background}{{1.1}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1}Activation Functions}{2}}
\newlabel{eq:elu}{{1}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.2}Hidden layers}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.3}Learning Rate Schedule}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Methodology}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}The Network Architecture}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Showing a visual representation of the convolutional neural network used.\relax }}{3}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{conv}{{1}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Convolutional Layer}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Fully Connected and Output prediction Layer}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}Pooling Layer}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Showing an example of a 2x2 max pool operation. The 3x3 equivalent operation was used throughout this investigation.\relax }}{4}}
\newlabel{pool}{{2}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.4}Dropout Layer}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.5}Batch Normalization}{5}}
\newlabel{batch_eq}{{2}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.6}Weights and Biases}{5}}
\newlabel{weights}{{2.1.6}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Experimental Procedure}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Activation Functions}{6}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Showing the combinations of activation functions, learning rates and training duration in Epochs, for the three rounds of experimentation.\relax }}{6}}
\newlabel{ac-overview}{{1}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Filter Sizes and Feature Maps}{6}}
\newlabel{filter-methods}{{2.2.2}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Regularization and Normalization}{7}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Showing the combinations of parameters used during the normalization and regularization investigative rounds.\relax }}{7}}
\newlabel{reg-table}{{2}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Network Training and Data Collection}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Activation Functions}{8}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Showing the final validation classification accuracy for the four activation functions, after 5 epochs, for different learning rates. The classification accuracies are reported as percentages.\relax }}{8}}
\newlabel{ac-table}{{3}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Showing a visual representation of the model used, from TensorBoard.\relax }}{9}}
\newlabel{ac_res}{{3}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Filter Sizes and Feature Maps}{9}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Showing the training execution time, in minutes, for 5 epochs for different kernel sizes and number of feature maps in the convolutional layers.\relax }}{9}}
\newlabel{filter-res}{{4}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Showing the training and validation set errors for different kernel sizes and number of feature maps in each convolutional layer. fs = 'Filter Size', nf = 'number of filters'. \relax }}{10}}
\newlabel{kernel_res}{{4}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Regularization and Normalization}{10}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Conclusion}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Showing the training set accuracy, and the difference between the traing and validation set accuracies for different normalization and regularization methods. \relax }}{11}}
\newlabel{norm_diff}{{5}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Showing the training and validation set accuracy of the regularized and normalized network on the left, and the non-normalized accuracies on the right. The plot on the right is the best performing trial from the first set of experiments. \relax }}{12}}
\newlabel{before_after}{{6}{12}}
\bibstyle{IEEEtran}
\bibdata{ref.bib}
\bibcite{long2015fully}{1}
\bibcite{Collobert}{2}
\bibcite{cifar10}{3}
